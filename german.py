#!/usr/bin/env python3
from collections import defaultdict
from os.path import exists
import re
import requests
from requests.exceptions import RequestException
from argparse import ArgumentParser
import tarfile
from json import dump
from lowercase_words import LOWERCASE_WORDS
from non_german_words import NON_GERMAN_WORDS
from company_trademark_names import COMPANY_TRADEMARK_NAMES
from people_names import PEOPLE_NAMES
from unwanted_words import UNWANTED_WORDS

SOURCE = "https://pcai056.informatik.uni-leipzig.de"
wiki_1M_url = f"{SOURCE}/downloads/corpora/deu_wikipedia_2021_1M.tar.gz"
news_1M_url = f"{SOURCE}/downloads/corpora/deu_news_2021_1M.tar.gz"
FILE_NAME_PATTERN = "deu_{type}_{year}_{amount}.tar.gz"
URL_PATTERN = f"{SOURCE}/downloads/corpora/{{file_name}}"


def download_file(url: str, file_name: str) -> None:
    """Download archive to current folder"""
    try:
        r = requests.get(url, stream=True)
    except RequestException as e:
        raise SystemExit(e)
    r.raise_for_status()

    with open(f"./{file_name}", "wb") as f:
        f.write(r.raw.read())


def words_from_file_in_archive(file_name: str, archive_name: str) -> dict:
    """Extract words file from archive and return it as dict

    The word will be the key, the value will be the frequency

    This assumes every line looks like this:
    rank<tab>word<tab>word frequency<newline>
    1	der	499367
    """
    words = defaultdict(int)
    with tarfile.open(archive_name, mode="r:gz") as tar:
        words_file = tar.extractfile(file_name)

        for line in words_file.readlines():
            line_list = line.decode("utf-8").strip().split("\t")
            words[line_list[1].strip()] += int(line_list[2])

    return words


def filter_words(words: dict, verbose: bool) -> list:
    """Sanatize words dict in multiple ways and return list ordered by frquency

    - Remove everything not a german latter
      (exclamation points, kommas, foreign characters)
    - Remove one letter words, like "m" (wtf?)
    - Remove non german words
    - Remove company and tradmark names
    - Remove names of people
    - Remove abbreviations (words with only capital letters)
    - Remove words with at least 1 capital letter in the middle
    - Remove unwanted words like insults
    - NOT: Remove words that only appeared once in the source material
      This is commented out due to lack of words for 250k
    - Fix capitalisation, since some words are only uppercase because of their
      position at the start of a sentence
    """
    filtered_words = defaultdict(int)

    NON_GERMAN_LETTERS = re.compile("[^a-zA-ZäöüÄÖÜßẞ]")
    ABBREVIATION_REGEX = re.compile("^[A-ZÄÖÜẞ]+$")
    CAPITAL_LETTER_IN_MIDDLE_REGEX = re.compile("^.+[A-ZÄÖÜẞ]")
    for word, frequency in words.items():
        # skip words with invalid "characters" like:
        # special characters or characters from another language
        if (
            re.search(NON_GERMAN_LETTERS, word)
            or len(word) == 1  # one letter word
            or word in NON_GERMAN_WORDS
            or word in COMPANY_TRADEMARK_NAMES
            or word in PEOPLE_NAMES
            or re.search(ABBREVIATION_REGEX, word)
            or re.search(CAPITAL_LETTER_IN_MIDDLE_REGEX, word)
            or word in UNWANTED_WORDS
            # or frequency == 1                       # one time occurences
        ):
            continue

        # some words are only uppercase in the collection due to a new sentence
        # we treat them like they were lowercase
        if word in LOWERCASE_WORDS:
            word = word.lower()

        filtered_words[word] += frequency

    if verbose:
        print(
            "Removed",
            len(words) - len(filtered_words),
            (len(words) - len(filtered_words)) / len(words),
        )

    sorted_list = [
        word
        for word, _ in sorted(filtered_words.items(), key=lambda x: x[1], reverse=True)
    ]

    return sorted_list


def write_json_file(words: list, source_file_name: str, count: int, name: str):
    json_dict = {
        "name": name,
        "_comment": (
            "Sourced from https://wortschatz.uni-leipzig.de/de/download/German "
            "generated by https://github.com/Syphdias/monkeytype-generate-german "
            f"from {source_file_name}"
        ),
        "leftToRight": True,
        "bcp47": "de-DE",
        "words": sorted(words[:count]),
    }

    with open(f"{name}.json", "w") as f:
        dump(json_dict, f, indent=2, ensure_ascii=False)


def main(args):
    archive_name = FILE_NAME_PATTERN.format(
        type=args.type, amount=args.amount, year=args.year
    )
    url = URL_PATTERN.format(file_name=archive_name)

    if not exists(archive_name) or not tarfile.is_tarfile(archive_name):
        download_file(url, archive_name)

    # read directly from requests?
    # tar = tarfile.open(fileobj=r.raw, mode="r:gz")

    source_file = (
        f"deu_{args.type}_{args.year}_{args.amount}/"
        f"deu_{args.type}_{args.year}_{args.amount}-words.txt"
    )
    words = words_from_file_in_archive(source_file, archive_name)

    words = filter_words(words, args.verbose)

    write_json_file(words, source_file, 200, "german")
    write_json_file(words, source_file, 1_000, "german_1k")
    write_json_file(words, source_file, 10_000, "german_10k")
    write_json_file(words, source_file, 250_000, "german_250k")
    write_json_file(words, source_file, len(words), "german_max")


if __name__ == "__main__":
    parser = ArgumentParser()
    parser.add_argument(
        "type",
        choices=["wikipedia", "news"],
    )
    parser.add_argument(
        "amount",
        nargs="?",
        choices=["10k", "30k", "100k", "300K", "1M"],
        default="1M",
    )
    parser.add_argument(
        "year",
        nargs="?",
        choices=["2021"],
        default="2021",
    )
    parser.add_argument(
        "--verbose",
        "-v",
        action="count",
        default=0,
    )
    args = parser.parse_args()

    main(args)
